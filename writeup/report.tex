\documentclass[times,11pt]{article}
%\usepackage[top=1cm, bottom=1cm, left=2.5cm, right=2.5cm]{geometry}

\usepackage{amsmath,amsfonts,amsthm}
\usepackage[pdftex]{graphicx}
\usepackage{float}
\usepackage{url}
\usepackage{natbib}

\numberwithin{equation}{section}		% Equationnumbering: section.eq#
\numberwithin{figure}{section}			% Figurenumbering: section.fig#
\numberwithin{table}{section}				% Tablenumbering: section.tab#

\graphicspath{ {./images/} }

%%% MACROS %%%

\newcommand{\ltwonorm}[1]{\left|\left|{#1}\right|\right|}
\newcommand{\xvec}{\mathbf{x}}
\newcommand{\HW}{\textsc{Hogwild!}}
\newcommand{\RR}{\mathbb{R}}
=======
\newcommand{\blocks}{\mathcal{B}}
\newcommand{\neighs}[1]{\mathcal{N}\left({#1}\right)}

%%%%%%%%%%%

\begin{document}

\title{Cache-Friendly Shuffles for Asynchronous Stochastic Optimization}
\author{Maximilian Lam, Horia Mania, and Maxim Rabinovich}

\maketitle
\section{Introduction}

In modern machine learning, many problems can be cast as instances of optimization, where the goal is to minimize some loss
function encoding the system's errors on training data. The ubiquity of such problems has led to substantial interest in making large-scale
optimization as efficient as possible, and a substantial portion of this work has focused on developing asynchronous algorithms that can under certain regimes
achieve near-ideal speedups in shared-memory systems.

% Introduce Hogwild.
So called {\it stochastic} optimization algorithms have found particular success in this regard. These algorithms, of which stochastic gradient descent (SGD) is the simplest example, operate
on problems whose loss functions decompose as a sum over datapoints:
\begin{align*}
F\left(x\right) & = \sum_{i = 1}^{n} f_{i}\left(x\right) .
\end{align*}
SGD then proceeds by iteratively pulling out the loss function $f_{i}$ for a specific datapoint, updating the model $x$ based on that data point alone, and then moving on to the next one.
SGD and its close cousins have the advantage of being very simply parallelizable. Indeed, the Hogwild algorithm~\citep{} simply spawns $p$ threads to perform the single data point updates in parallel,
allowing them to all update the model without any synchronization whatsoever. Perhaps surprisingly, this lock-free asynchronous approach works well for many problems, and since it avoids communication entirely,
we might hope to achieve ideal speedups and thereby solve even the largest optimization problems facing machine learning practitioners.

% Theory.

% Explain systems issues at high level.
Unfortunately, the simple story fails for a number of reasons, many of them systems-related. As the number of threads becomes large, the architecture of the machine
imposes limitations and tradeoffs that must be taken into account. For instance, many machines with sufficiently many cores to support parallelism beyond approximately $16$
threads organize their cores in a non-uniform memory access (NUMA) architecture, meaning that implementations of Hogwild must either cope with potentially long cross-NUMA node
accesses to the memory storing the model or alter the algorithm so that cores on a given node usually only access memory on that node. And when moving to multiple machines,
alterations to the algorithm become not only desirable but absolutely necessary.

% Discuss related work (e.g. DimmWitted, Downpour) on multi-node/multi-machine setting
A great deal of work has gone into understanding the statistical and computational properties of multi-node and multi-machine versions of the algorithm~\citep{}. The recent DimmWitted system
explored the design space of Hogwild-type algorithms in significant detail, tuning several knobs like: how much of the data each thread used; whether independent copies of the model were stored by each thread,
or only on each node, or whether only a single global version of the model was maintained at all times; and how often threads received updated versions of other threads' models in the case of model replication. They
found substantial differences in performance across these settings, and identified important computational-statistical tradeoffs to guide further work. In prior work,~\citet{} had undertaken a similar study in the
rather different multi-machine setting.

% Pivot to single-node case (ref Jellyfish, Buckwild).
Comparatively little attention has been paid to the simpler setting of a single node, despite the fact that, even there, failing to take the memory access pattern into account can limit the speedup from parallelization
both on the individual node in isolation and in the context of larger multi-node or multi-machine systems. A notable exception to this general trend is the Jellyfish system for large-scale matrix factorization, which employs a 
blocking scheme to ensure that no two threads access the same parameters in any given epoch---a design choice that also ensures a high degree of cache locality within each thread's memory accesses, since each thread's 
assigned block depends only on a fraction of the total model parameters~\citep{}. And more recently, further support for the idea that memory access optimizations can greatly increase the efficiency of optimization algorithms
has come from work showing that low-precision arithmetic variants of the Hogwild algorithm can achieve substantially better speedups than their high-precision predecessors~\citep{}.

% Introduce our approach
In this report, we investigate a general-purpose methodology for optimizing the memory access efficiency of Hogwild-style algorithms. Building on the Jellyfish results, we propose to group datapoints into blocks that each access only a 
limited subset of the parameters and to use these blocks constrain the order in which threads access datapoints. We hypothesize that, if the blocking is done well, this strategy will have the effect of increasing cache hits on model parameters within a single thread.
Further, if different threads are further required to iterate through the blocks in different orders, we expect it to have the additional beneficial effect of reducing conflicting model accesses across threads, thereby also decreasing false sharing due to cache coherence
effects.

The remainder of this report elaborates on our approach and provides evidence for these hypotheses. In Section~\ref{sec:alg-spec}, we explain the details of our proposed algorithm and introduce the bipartite datapoint-parameter dependence graph that underlies it. In Section~\ref{sec:memory-model}, we provide a simple 
two-level memory model analysis of the \HW~ algorithm and frame the problem of optimally blocking datapoints as a graph partitioning problem in the dependence graph. In Section~\ref{sec:ls}, we investigate how the structure of the datapoint-parameter graph affects the potential gains from 
effective blocking. In Section~\ref{sec:w2v}, we go on to show that a particular blocking strategy based on iterated min-cut can achieve substantial gains, both in runtime and speedup, on a real machine learning problem, namely the learning task for the popular Word2Vec model for word embeddings~\citep{}. Finally, in 
Section~\ref{sec:conc}, we reflect on our analysis and propose several avenues for further work.

\section{Block-Constrained Shuffling}\label{sec:alg-spec}

Our approach to making \HW~ memory access efficient hinges on constraining the order in which the algorithm processes data points. Typically, each thread
in the algorithm processes the data points in a randomly selected thread-specific ordering, meaning that the data
point processed by a thread at some iteration $j$ need not share any parameters with the data point processed at iteration $j - 1$ or $j + 1$. We aim to remedy this defect.

Specifically, we aim to group the datapoints into blocks that mostly depend on the same parameters, and such that each block depends only a comparatively small number of 
distinct parameters. Given this blocking, we constrain the order in which threads process datapoints to be consistent with the blocking. Here consistency means that, as the thread iterates through
the data, once it has processed a data point from some block, it will not process any more data points outside that block until it has finished with the entire block.

We can make the algorithm specification more precise as follows. Let the blocks of data points be given by $\blocks = \lbrace B_{1}, \dots, B_{K} \rbrace$, where each $B_{k} = \lbrace i_{k1}, \dots, i_{km_{k}}\rbrace$ is a set of data point indices. Then we assume that the sequential ordering
$I_{p} = \left(i_{p1}, \dots, i_{pn}\right)$ according to which thread $p$ processes data points has the form $\left(I_{pa_{1}}, \dots, I_{pa_{K}}\right)$, where each $I_{pa_{k}}$ is an ordering of the indices in $B_{k}$. 

We point out, anticipating the analysis in Section~\ref{sec:memory-model}, that the problem of choosing a good blocking bears a strong resemblance to the graph partitioning problem that arises in sparse matrix-vector multiply~\citep{}. The difference is that the dependence graph for our problem
links datapoints to parameters and therefore has a bipartite structure. The blocking problem is then simply a graph partitioning problem on one side of this graph. 

To be more precise, we write each $f_{i}$ as a function of a set $S_{i}$ of $s_{i}$ parameters, out of $d$ model parameters total. To highlight this structure, we write $f_{i}\left(\xvec\right) = f_{i}\left(\xvec_{S_i}\right)$. Datapoint $i$ is then connected to all the parameters in $S_{i}$,
with edges interpreted as undirected. As a consequence, parameter $j$ is connected to all data points $i$ such that $j \in S_{i}$. Figure~\ref{} gives a schematic of such a graph.

\section{A Simplified Memory Model Analysis}\label{sec:memory-model}

Simplified memory models can provide useful guidance when optimizing algorithm performance, as the case of matrix-matrix multiply illustrates~\citep{}. In this section, we use a two-level memory model to estimate the computational intensity of the algorithm
in terms of the blocking structure and use the resulting estimate to formalize the problem of finding an optimally memory access efficient blocking as a graph partitioning problem in the datapoint-parameter graph described in Section~\ref{sec:alg-spec}.

Concretely, assume the memory hierarchy has two levels, fast and slow, where fast memory contains sufficient storage for $M$ model parameters and has zero access cost. We assume further that reads and writes have equal cost and that the computational effort required to obtain the model update based on datapoint $i$ is proportional to $s_{i}$, say equal to $cs_{i}$ for some absolute constant $c > 0$. 

Note that under this framework, a model update based on datapoint $i$ requires $s_{i}$ parameter reads to compute the update, $cs_{i}$ steps of computation (e.g. flops) to compute the update, and $s_{i}$ parameter writes to actually perform the update. 

Now let the order in which a thread performs updates based on data points be $I = \lbrace i_{1}, \dots, i_{n}\rbrace$. To process data point $i_{j}$, the
algorithm needs to read in some number $r_{j}\left(I\right)$ of model parameters to perform the update; the remaining $s_{i_j} - r_{j}\left(I\right)$ are
already in fast memory and therefore cost nothing to access. The total number of reads is therefore given by $\sum_{i = 1}^{n} s_{i} - \sum_{j = 1}^{n} r_{j}\left(I\right)$, while the total number of writes is given by $\sum_{i} s_i$. The computational intensity can therefore be written as
\begin{align}
q\left(I\right) & = \frac{c\sum_{i = 1}^{n} s_{i}}{2\sum_{i = 1}^{n} s_{i} - \sum_{j = 1}^{n} r_{j}\left(I\right)} . 
\end{align}
The problem of maximizing $q$ is therefore equivalent to the problem of minimizing the sum
\begin{align}
r\left(I\right) & = \sum_{j = 1}^{n} r_{j}\left(I\right) . 
\end{align}

In the context of the blocking algorithm, we can bound $r\left(I\right)$ as follows. Let the blocking structure be given by $\blocks = \lbrace B_{1}, \dots, B_{K} \rbrace$. Let $m_{k}$ be the number of distinct parameters touched by block $k$, or, more formally, $m_{k} = \left|\cup_{i \in B_{k}} S_{i}\right|$, which is also
equal to the number of neighbors of $B_{k}$, $\neighs{B_{k}}$, in the datapoint-parameter dependence graph. Provided each $m_{k}$ is bounded by the fast memory size $M$, we have
\begin{align}
r\left(I\right) & \leq \sum_{k = 1}^{K} m_{k} . 
\end{align}
Writing $m_{k} = \left|\neighs{B_{k}}\right|$, we thus find that the graph partitioning problem we face can be formulated as
\begin{align}\label{eq:comb-opt}
\min \sum_{k = 1}^{K} \left|\neighs{B_{k}}\right| ~~ \text{subject to}~~ \left|\neighs{B_{k}}\right| \leq M,~ 1 \leq k \leq K .
\end{align}

As this problem bears a strong resemblance to the NP-hard set cover problem~\citep{}, we do not attempt to solve it exactly. Instead, we investigate three heuristics for solving it:
\begin{enumerate}
\item
{\bf Truncated breadth-first search.} In this approach, we do not fix the number of blocks in advance. Instead, we iterate through the data points and whenever we encounter one that has not yet been assigned a block, we start a breadth-first search for datapoints to put in a new block including the data point initiating the 
search. Whenever the search encounters a data point that has not yet been assigned to a block, it adds it to the new block and enqueues all of its neighbors that have not themselves been assigned to a block. The search halts either when the queue is empty, or when the number of data points assigned to he new block
exceeds a client-specified threshold.

\item
{\bf Greedy streaming.} In this approach, we fix the number of blocks before the algorithm starts. Then, we iterate through the data points and at each step, add the next data point to the block $B_{k}$ that makes the objective in~\eqref{eq:comb-opt} as small as possible.

\item
{\bf Iterated min-cut.} In this approach, we again fix the number of blocks in advance, specifically setting $K = 2^{\kappa}$, where $\kappa \geq 0$ is a depth parameter. We then perform $\kappa$ recursive min-cuts and keep the bottom level of the resulting hierarchical blocking.
\end{enumerate}

\section{Least Squares}\label{sec:ls}

To test our hypotheses about the impact of cache effects on the speed of asynchronous optimization algorithms and to show the benefits of our proposed method, we implemented a \HW{} least squares solver. More precisely, we minimize the loss 
\begin{equation}
\label{eq:LS}
F(\xvec) = \sum_{i = 1}^n \ltwonorm{\langle a_i, \xvec \rangle - b_i}^2
\end{equation}
via asynchronous stochastic optimization, where $a_i, \xvec \in \RR^d$ and $b_i\in \RR$. 
At each iteration we sample a data-point $(a_i, b_i)$, we read the coordinates of the model $\xvec$ that belong to the support of the vector $a_i$, and compute the gradient $2(\langle a_i, \xvec\rangle - b_i)a_i$ and update the model $\xvec$ in the shared memory.

The benefits of studying this problem are twofold. Firstly, it is easy to describe the bipartite graph described above. Each data point $(a_i,b_i)$ is connected to the coordinates of $\xvec$ that belong to the support of $a_i$. This property allows us to easily generate synthetic data with different structures of the bipartite graph and test what properties of the bipartite graph impact performance. Secondly, the analysis of this problem yields insights for any optimization problem of the form 
\begin{equation}
F(\xvec) = \sum_{i = 1}^n \ell (\langle a_i, \xvec\rangle, b_i),
\end{equation}
where $\ell \colon \RR \times \RR \rightarrow \RR$ is some loss function. For example, training a linear SVM requires optimizing such an objective. 


\subsection{Graph Structures Considered}

All the data sets we generated have $n = 3\cdot 10^6$ data-points and $d = 10^4$ parameters. The non-zero entries of the data-points $(a_i, b_i)$ have been sampled uniformly from the interval $(-1,1)$.  We made these choices because for datasets of this size the standard \HW{} method achieves good speedup for up to $12$ threads, allowing us to observe what benefits our method brings in a setting where \HW{} performs well. 

\subsubsection*{Uniform}

To generate a datapoint $(a_i, b_i)$ under this model we sample uniformly at random and without-replacement a set of $k$ parameters (in our experiments we chose $k = 20$) out of the total $10^4$ parameters of $a_i\in \RR^{10^4}$ and assign them a value sampled uniformly at random from $(-1,1)$. The real value $b_i$ is uniformly sampled from $(-1,1)$. 


\subsubsection*{Preferential-Attachment}

A dataset under this model is generated sequentially. Firstly we connect the datapoint $(a_1, b_1)$ to a set of $k$ parameters chosen uniformly at random without replacement. Then, for each new datapoint $(a_i, b_i)$ we connect it to a $k$ parameters chosen without replacement with a probability proportional to their current degree in the bipartite graph (i.e. the number of datapoints that depend on them so far). The datapoints are shuffled at the end.


\subsubsection*{Stochastic Block Model}

Under this model we group the parameters into disjoint sets of $k$-parameters. Denote by $b$ the number of groups of parameters obtained. Then, for each group of parameters choose without replacement $\lfloor \frac{n}{b}\rfloor$ data-points and connect each of them to all of the parameters in the group. Finally, connect each data point to $k'$-parameters chosen uniformly from the blocks that do not correspond to that datapoint. Under this model there are ground 

\subsection{Experimental results}

We implemented the \HW{} algorithm for optimizing objective \eqref{eq:LS} in $C++$ and ran the experiments on 
We ran our experiments on the Edison compute nodes which feature two twelve core 2.4 GHz processors. 
However, we used up to twelve threads and pinned them to individual cores of one NUMA node to avoid effects of NUMA.








\subsection{Discussion}















\section{Word Embeddings}\label{sec:w2v}

\subsection{Problem statement}
In the word embeddings problem, given context counts $X_{w,w'}$ we want to find word vectors
$v_{w} \in \mathbb{R}^{k}$ that minimizes the loss:
\begin{align*}
\min_{v,C}\sum_{w,w'}X_{w,w'} \left(\log(X_{w,w'}) - \ltwonorm{v_w+v_{w'}}^2 - C\right)^2
\end{align*}

\subsection{Experimental evaluation}
\subsubsection{Methodology}

We ran our experiments on the Edison compute nodes which feature two
twelve core 2.4 GHz processors. However, we used only up to twelve
cores/threads to avoid effects of NUMA. Word vectors were length 100
double arrays.
\\\\
We used the first $10^9$ bytes of English Wikipedia from
http://mattmahoney.net/dc/textdata as corpus data. After running the text
preprocessing script supplied by the link, we computed co-occurrence
counts of pairs of distinct words to create the parameter dependence graph.
This graph was then fed into gpmetis, computing a min-k-cut partitioning to
create a cache-friendly ordering of the datapoints. k was set such that each
block of k datapoints would reference just enough word vectors to fit into the
L1-cache.
\\\\
Hogwild was then run on the permuted co-occurrence graph generated by gpmetis,
maintaining the same ordering throughout execution. Although we experimented with
both data sharding and no-data sharding, only results from data sharding are presented.
To test hogwild without a cache-friendly shuffle, we randomly shuffled the datapoints before execution.
\\\\
We also ran the experiments on subsets of the corpus, repeating the
procedure on the first $\%10$, $\%25$, $\%50$ and $\%75$ of the corpus data.
In the full corpus data, there were $~200,000$ word vectors, and $~30,000,000$
datapoints.

\subsubsection{Results}
\noindent We achieve between $\%40-\%50$ speedup over regular hogwild (non-cache-friendly hogwild),
measuring runtime to a fixed number of epochs.
\begin{figure}[H]
\includegraphics[width=11cm,height=11cm,keepaspectratio]{w2v_speedup_plot.pdf}
\end{figure}
\noindent Furthermore, the speedup is maintained on different subsets and sizes of the data.
\begin{figure}[H]
\begin{center}
\includegraphics[width=11cm,height=11cm,keepaspectratio]{w2v_problem_size_time_plot.pdf}
\end{center}
\end{figure}
\noindent Additionally, convergence of loss is not adversely affected.
\begin{figure}[H]
\begin{center}
\includegraphics[width=11cm,height=11cm,keepaspectratio]{w2v_time_loss_plot.pdf}
\end{center}
\end{figure}

\subsection{Discussion}

A $\%40-\%50$ runtime gain over regular hogwild is a result of keeping
at least one length 100 double array in the L1-cache between
stochastic gradient calls. In a non-cache-friendly permutation, each
of the two vectors visited by a datapoint is typically not in the
cache, incurring two vectors worth of cache misses per
datapoint. After running min-k-cut on the parameter dependence graph,
we found that each block of k datapoints references around k distinct
vectors. Thus, in a cache-friendly permutation, one of the vectors
referenced by a datapoint is already in the L1-cache from the previous
stochastic gradient call. So a cache-friendly permutation incurs only
one vectors worth of cache misses per datapoint, naturally leading to
a $\%40-\%50$ reduction in runtime.
\newpage

\section{Conclusion and future directions}\label{sec:conc}
By ordering datapoints so that model parameters are accessed in a cache friendly manner, we achieve substantial runtime reductions.

% Conclusion for LS

% Conclusion for W2V
For word embeddings, we achieve a $\%40-\%50$ runtime gain over regular hogwild by ordering the datapoints via min-k-cut.

% Overall Conclusion
From these results we believe that using a cache friendly shuffling of datapoints is a promising approach to reducing the runtime of stochastic optimization methods.

% Future directions
The runtime gains achieved by using a cache-friendly shuffle is highly
contingent on the structure of the parameter depedence graph and on
the method used to permute the datapoints. Thus, one possible
direction to explore is the behavior of cache-friendly shuffles on
different graph structures and problem types. We may find that graph
properties such as sparsity, etc, have a nontrivial effect on the
efficacy of cache-friendly shuffles.
\\\\
Different shuffling methods may be another topic of exploration, particularly
the tradeoff between computational efficiency and shuffle
quality. Investigating different greedy methods and heuristics for
greedy shuffles may reveal computationally efficient methods for
generating a quality cache shuffe. On the other hand, it may also be
interesting to see how effective optimal shuffles are, perhaps by
generating them through some sort of integer linear programming
routine.
\\\\
Finally, a shuffling that takes advantage of all levels of cache may yield further runtime gains.
Thus, a cache-oblivious shuffling method may be yet another area to explore.

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
